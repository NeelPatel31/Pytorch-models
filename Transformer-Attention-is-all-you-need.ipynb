{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0+w/WnFhNoiyh8Q5MZYDA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"EsZBL08_MQNp","executionInfo":{"status":"ok","timestamp":1742704125326,"user_tz":-330,"elapsed":95,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","  def __init__(self, embed_size, heads):\n","    super(SelfAttention, self).__init__()\n","    self.embed_size = embed_size\n","    self.heads = heads\n","    self.head_dim = embed_size // heads\n","\n","    assert (self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n","\n","    self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","    self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","    self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","    self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n","\n","  def forward(self, values, keys, query, mask):\n","    N = query.shape[0]\n","    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","    # Split embedding into self.heads pieces\n","    # (N, len, embed_size) -> (N, len, heads, head_dim)\n","    values = values.reshape(N, value_len, self.heads, self.head_dim)\n","    keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","    queries = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","    values = self.values(values)\n","    keys = self.keys(keys)\n","    queries = self.queries(queries)\n","\n","    energy = torch.einsum('nqhd,nkhd->nhqk', [queries, keys]) # (N, heads, query_len, key_len)\n","\n","    if mask is not None:\n","      energy = energy.masked_fill(mask == 0, float('-1e20'))\n","\n","    attention = torch.softmax(energy/(self.embed_size**0.5), dim=3)\n","\n","    # Attention shape: (N, heads, query_len, key_len)\n","    # Values shape: (N, value_len, heads, heads_dim)\n","    # (N, query_len, heads, heads_dim)\n","    out = torch.einsum('nhql,nlhd->nqhd', [attention, values])\n","    out = out.reshape(N, query_len, self.heads*self.head_dim)\n","    out = self.fc_out(out)\n","    return out"],"metadata":{"id":"8VARg_62Qpns","executionInfo":{"status":"ok","timestamp":1742709780142,"user_tz":-330,"elapsed":73,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","  def __init__(self, embed_size, heads, dropout, forward_expansion):\n","    super(TransformerBlock, self).__init__()\n","    self.attention = SelfAttention(embed_size, heads)\n","    self.norm1 = nn.LayerNorm(embed_size)\n","    self.norm2 = nn.LayerNorm(embed_size)\n","\n","    self.feed_forward = nn.Sequential(\n","        nn.Linear(embed_size, forward_expansion*embed_size),\n","        nn.ReLU(),\n","        nn.Linear(forward_expansion*embed_size, embed_size)\n","    )\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, value, key, query, mask):\n","    attention = self.attention(value, key, query, mask)\n","\n","    x = self.dropout(self.norm1(attention + query))\n","    forward = self.feed_forward(x)\n","    out = self.dropout(self.norm2(forward + x))\n","    return out"],"metadata":{"id":"Y66G2YU-RJt-","executionInfo":{"status":"ok","timestamp":1742709780158,"user_tz":-330,"elapsed":3,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","  def __init__(self,\n","               src_vocab_size,\n","               embed_size,\n","               num_layers,\n","               heads,\n","               device,\n","               forward_expansion,\n","               dropout,\n","               max_length\n","  ):\n","    super(Encoder, self).__init__()\n","    self.embed_size = embed_size\n","    self.device = device\n","    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n","    self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","    self.layers = nn.ModuleList(\n","        [\n","            TransformerBlock(\n","                embed_size,\n","                heads,\n","                dropout=dropout,\n","                forward_expansion=forward_expansion\n","            )\n","            for _ in range(num_layers)\n","        ]\n","    )\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, mask):\n","    N, seq_length = x.shape\n","    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","\n","    out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n","\n","    for layer in self.layers:\n","      out = layer(out, out, out, mask)\n","\n","    return out"],"metadata":{"id":"uy_hN-8PfO6g","executionInfo":{"status":"ok","timestamp":1742709780194,"user_tz":-330,"elapsed":27,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class DecoderBlock(nn.Module):\n","  def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","    super(DecoderBlock, self).__init__()\n","    self.attention = SelfAttention(embed_size, heads)\n","    self.norm = nn.LayerNorm(embed_size)\n","    self.transformer_block = TransformerBlock(\n","        embed_size, heads, dropout, forward_expansion\n","    )\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, value, key, src_mask, trg_mask):\n","    attention = self.attention(x, x, x, trg_mask)\n","    query = self.dropout(self.norm(attention+x))\n","    out = self.transformer_block(value, key, query, src_mask)\n","    return out"],"metadata":{"id":"wbL1p1bDg1tt","executionInfo":{"status":"ok","timestamp":1742709780207,"user_tz":-330,"elapsed":4,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self,\n","               trg_vocab_size,\n","               embed_size,\n","               num_layers,\n","               heads,\n","               forward_expansion,\n","               dropout,\n","               device,\n","               max_length):\n","    super(Decoder, self).__init__()\n","    self.device = device\n","    self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n","    self.position_embedding = nn.Embedding(max_length, embed_size)\n","    self.layers = nn.ModuleList(\n","        [\n","            DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n","            for _ in range(num_layers)\n","        ]\n","    )\n","    self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, enc_out, src_mask, trg_mask):\n","    N, seq_length = x.shape\n","    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","    x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n","\n","    for layer in self.layers:\n","      x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n","\n","    out = self.fc_out(x)\n","    return out\n","\n","class Transformer(nn.Module):\n","  def __init__(self,\n","               src_vocab_size,\n","               trg_vocab_size,\n","               src_pad_idx,\n","               trg_pad_idx,\n","               embed_size=256,\n","               num_layers=6,\n","               forward_expansion=4,\n","               heads=8,\n","               dropout=0,\n","               device='cpu',\n","               max_length=100\n","  ):\n","    super(Transformer, self).__init__()\n","    self.encoder = Encoder(\n","        src_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length\n","    )\n","\n","    self.decoder = Decoder(\n","        trg_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        forward_expansion,\n","        dropout,\n","        device,\n","        max_length\n","    )\n","\n","    self.src_pad_idx = src_pad_idx\n","    self.trg_pad_idx = trg_pad_idx\n","    self.device=device\n","\n","  def make_src_mask(self, src):\n","    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","    # (N, 1, 1, src_len)\n","    return src_mask.to(self.device)\n","\n","  def make_trg_mask(self, trg):\n","    N, trg_len = trg.shape\n","    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n","        N, 1, trg_len, trg_len\n","    )\n","    return trg_mask.to(self.device)\n","\n","  def forward(self, src, trg):\n","    src_mask = self.make_src_mask(src)\n","    trg_mask = self.make_src_mask(trg)\n","\n","    enc_src = self.encoder(src, src_mask)\n","    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n","    return out"],"metadata":{"id":"cqjUedRshqFO","executionInfo":{"status":"ok","timestamp":1742709780217,"user_tz":-330,"elapsed":8,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n","trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n","\n","src_pad_idx = 0\n","trg_pad_idx = 0\n","src_vocab_size = 10\n","trg_vocab_size = 10\n","model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n","out = model(x, trg[:,:-1])\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVY2WO5fkkf3","executionInfo":{"status":"ok","timestamp":1742709780283,"user_tz":-330,"elapsed":60,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}},"outputId":"07fc7424-680f-4049-fcec-a660548d5a29"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 7, 10])\n"]}]},{"cell_type":"code","source":["out"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-YWiAH3k4aD","executionInfo":{"status":"ok","timestamp":1742709780310,"user_tz":-330,"elapsed":28,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}},"outputId":"791a0095-19ca-4d7f-b6e4-4f6c5f268a44"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.8983,  0.7658,  1.0083,  0.3012, -0.7434,  0.6354, -0.0729,\n","          -0.3913, -0.7436, -1.5433],\n","         [-0.2656,  0.1374, -0.1788,  0.2311,  0.2549,  0.3430,  0.2369,\n","           0.9063, -0.7722,  0.0567],\n","         [-0.9389, -0.0365,  1.3003,  0.3068,  0.1312,  0.4618,  0.0174,\n","          -0.1831, -0.5926,  0.0258],\n","         [-0.8179,  0.1909,  0.2076,  0.5756,  0.5377,  0.2627,  0.4117,\n","           0.9175,  0.3599,  0.1604],\n","         [-0.7408,  0.9745,  0.9814,  0.1676, -0.2620,  1.2020,  0.3641,\n","           0.3088, -0.1438, -0.5299],\n","         [-0.2455,  0.7037,  0.9652,  0.9225,  0.1833,  0.4665,  0.9650,\n","           0.4382, -0.2079, -0.5700],\n","         [-0.3213,  0.3019,  0.5731,  0.9077,  0.3636,  0.1180,  0.2583,\n","           0.7721, -0.1934, -0.3822]],\n","\n","        [[-0.8569,  0.6146,  1.0492,  0.6053, -0.4283,  0.4605, -0.3821,\n","          -0.2456, -0.5175, -1.5736],\n","         [-0.5528,  0.3134,  0.5909,  0.8559,  0.5680,  0.5092, -0.1578,\n","           1.2725, -0.1694, -0.0604],\n","         [-0.2224,  0.3497,  1.2411,  0.1640,  0.1652,  0.1382,  0.0466,\n","           0.6361, -0.5083, -0.6791],\n","         [-0.0489,  0.4634,  0.1713,  1.4637,  0.5189,  0.0631,  0.4999,\n","           1.2701,  0.3110, -0.2929],\n","         [-0.2302,  0.1231,  0.8323,  0.0291,  0.2969,  1.5076, -0.1503,\n","          -0.2708,  0.4811, -0.3644],\n","         [ 0.1460,  0.2245,  0.5258,  0.8787,  0.0060, -0.2220,  0.6320,\n","           0.6166, -0.0898, -0.0393],\n","         [ 0.1042,  0.0772,  0.8824,  0.1542,  0.5828,  0.1575, -0.3465,\n","           0.3901,  0.2481, -0.6615]]], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":[],"metadata":{"id":"pcpzixhulaxw","executionInfo":{"status":"ok","timestamp":1742709780313,"user_tz":-330,"elapsed":7,"user":{"displayName":"Neel D Patel","userId":"09865007007989319095"}}},"execution_count":31,"outputs":[]}]}